# USAGE# Program partially adopted and inspired from Adrian at PyImageSearch. The Objective of the program to isolate the iris# and localize or identify the cornea of the eye. The ultimate objective of the program is to integrate these features# to work with a thermal videos and images. The thermal integration aspect of the code is currently under development.# To successfully run the code you will need to have either a webcam or use the .seq file. However for a better# performance of the code, i recommend webcam.# To run the code please type... python iTemp_facial_extract.py --from scipy.spatial import distancefrom imutils.video import VideoStreamfrom imutils import face_utilsimport imutilsimport dlibimport cv2import numpy as npimport argparseimport timefrom PIL import Imageimport csv################################################################## Construct the argument parse and parse the argumentsap = argparse.ArgumentParser()# ap.add_argument("-p", "--shape-predictor", required=True)# ap.add_argument("-r", "--camera", type=int, default=-1)ap.add_argument("-seq", "--seq", type=bool, default=False, help='If True, process .seq file.')args = vars(ap.parse_args())print(args)#################################################################global leftEyeHull, rightEyeHulldef eye_aspect_ratio(eye):	A = distance.euclidean(eye[1], eye[5])	B = distance.euclidean(eye[2], eye[4])	C = distance.euclidean(eye[0], eye[3])	ear = (A + B) / (2.0 * C)	return ear# Initialize dlib's face detector (HOG-based) and create facial landmark predictordetector = dlib.get_frontal_face_detector()predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")# predictor = dlib.shape_predictor(args["shape_predictor"])# grab the indexes of the facial landmarks for the left and# right eye, respectively(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS["left_eye"](rStart, rEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS["right_eye"]def is_contour_bad(c):	# approximate the contour	peri = cv2.arcLength(c, True)	approx = cv2.approxPolyDP(c, 0.02 * peri, True)	# the contour is 'bad' if it is not a rectangle	return not len(approx) == 4def eye_detection(frame_im, overlay):	# loop over the face parts individually	sides = 0	for (name, (i, j)) in face_utils.FACIAL_LANDMARKS_IDXS.items():		if name == "left_eye" or name == "right_eye":			(x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))			roi_1 = frame_im[y:y + h, x:x + w]			roi_1 = imutils.resize(roi_1, width=250, inter=cv2.INTER_CUBIC)			roi_2 = overlay[y:y + h, x:x + w]			roi_2 = imutils.resize(roi_2, width=250, inter=cv2.INTER_CUBIC)			result = np.bitwise_and(roi_1, roi_2)			cv2.imwrite("Images/OS_Im_{}.png".format(sides), result)			cv2.imshow('Eye_Im', result)			# image = Image.open("./Images/OS_Im_{}.png".format(sides), 'r')			# width, height = image.size			# pixel_values = list(image.getdata())			# if image.mode == 'RGB':			# 	channels = 3			# elif image.mode == 'L':			# 	channels = 1			# else:			# 	print("Unknown mode: %s" % image.mode)			# 	return None			# pixel_values = np.array(pixel_values).reshape((width, height, channels))			# with open('./Images/pixel_data.csv', mode='w') as pixel_data:			# 	pix = csv.writer(pixel_data, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)			# 	pix.writerows(pixel_values)			sides += 1			cv2.waitKey(0)# --------------------MAIN-------------------------------# initialize the frame counters and the total number of blinksimg_counter = 0COUNTER = 0vs = VideoStream(src=0).start()while True:	frame = vs.read()	if frame is None:		print("yes")		break	frame = imutils.resize(frame, width=600)	gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)	rects = detector(gray, 0)  # detect face	key = cv2.waitKey(1) & 0xFF  # loop over the face detections	for rect in rects:		shape = predictor(gray, rect)		shape = face_utils.shape_to_np(shape)		leftEye = shape[lStart:lEnd]  # locate the left eye - block		rightEye = shape[rStart:rEnd]  # locate the right eye - block		leftEAR = eye_aspect_ratio(leftEye)  # got to def eye_aspect ratio		rightEAR = eye_aspect_ratio(rightEye)		ear = (leftEAR + rightEAR)/2.0		leftEyeHull = cv2.convexHull(leftEye)  # tight fitting convex boundary around the points or the shape.		rightEyeHull = cv2.convexHull(rightEye)		cv2.drawContours(frame, [rightEyeHull], -1, (0, 0, 0), 2)		cv2.drawContours(frame, [leftEyeHull], -1, (0, 0, 0), 2)		# if ear < 0.30:		# 	COUNTER += 1		# 	if COUNTER == 15:		# 		print('Open your eyes, Please!')		# else:		if ear >= 0.30 and key == 32:			# press space to take image			# loop over the (x,y) coordinates for the facial landmarks			# and draw them on the image			time.sleep(2)			img_name = "./Images/Img_frame_{}.png".format(img_counter)			cv2.imwrite(img_name, frame)			img = cv2.imread(img_name, 0)			img_counter += 1		else:			COUNTER += 1			if COUNTER >= 30:				print('Open your eyes, Please!')				COUNTER = 0	# show the frame - color	cv2.imshow("Frame", frame)  # Color Image	if img_counter >= 1:		frame_im = cv2.imread("./Images/Img_frame_{}.png".format(img_counter-1))		overlay = np.zeros(frame_im.shape, dtype="uint8")		# overlay = frame_im.copy()		frame_im = imutils.resize(frame_im, width=600)		cv2.drawContours(overlay, [rightEyeHull], -1, (255, 255, 255), -1)  # (158, 163, 32)		cv2.drawContours(overlay, [leftEyeHull], -1, (255, 255, 255), -1)  # 168, 100, 168		eye_detection(frame_im, overlay)  # def eye_detection()		# grayIm = cv2.cvtColor(frameIm, cv2.COLOR_BGR2GRAY)		break	# press q twice to exit	elif key == 27:  # ord("q"):		print("Escape hit, closing...")		break# vs.release()cv2.waitKey(0)cv2.destroyAllWindows()